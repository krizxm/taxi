{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fare amount is negative and it doesn't seem to be realistic\n",
    "#few longitude and lattitude entries are off\n",
    "#maximum passanger count is 208 which looks odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.dropna(how='any', axis=0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape #drop 376 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[train.fare_amount >=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape # drop 2454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[train.passenger_count >7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train[train.passenger_count <=7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape #drop 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[train.passenger_count ==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Passanger count is incorrectly populated\n",
    "#Taxi was not carrying any passanger, may be taxi was used for goods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train[train.passenger_count >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape # drop 195k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store the minimum and maximum of the longitude and latitude from test data set and filter the train data set for those data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min(test.pickup_longitude.min(),test.dropoff_longitude.min()), \\\n",
    "max(test.pickup_longitude.max(),test.dropoff_longitude.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min(test.pickup_latitude.min(),test.dropoff_latitude.min()), \\\n",
    "max(test.pickup_latitude.max(),test.dropoff_latitude.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_within_test_boundary(df, BB):\n",
    "    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BB = (-74.5, -72.8, 40.5, 41.7)\n",
    "train = train[select_within_test_boundary(train, BB)]\n",
    "train.shape # drop 1,169,992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eda sample 2,000,000 obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sample = train.sample(n=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets see the distribution of fare amount \n",
    "train_sample.fare_amount.hist(bins=100, figsize = (16,8))\n",
    "plt.xlabel(\"Fare Amount\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets see the distribution of fare amount less than 100\n",
    "train_sample[train_sample.fare_amount <=100 ].fare_amount.hist(bins=100, figsize = (16,8))\n",
    "plt.xlabel(\"Fare Amount\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#There are few points between 40 and 60 dollars which has slightly high frequency and that could be airport trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sample[train_sample.fare_amount >100 ].fare_amount.hist(bins=100, figsize = (16,8))\n",
    "plt.xlabel(\"Fare Amount\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some of them might be outliers or few of them might be long distance trip from/to airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sample.passenger_count.hist(bins=10, figsize = (16,8))\n",
    "plt.xlabel(\"Passanger Count\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (16,8))\n",
    "sns.boxplot(x = train_sample.passenger_count, y = train.fare_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,2,figsize = [10,5])\n",
    "sns.countplot(train_sample[\"passenger_count\"], ax=ax[0])\n",
    "sns.countplot(test[\"passenger_count\"], ax=ax[1])\n",
    "ax[0].set_title(\"Train Set - Passenger Count\")\n",
    "ax[1].set_title(\"Test Set - Passenger Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "# time\n",
    "# distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_time_features(df):\n",
    "    df['pickup_datetime'] = df['pickup_datetime'].str.replace(\" UTC\", \"\")\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df['hour_of_day'] = df.pickup_datetime.dt.hour\n",
    "    df['week'] = df.pickup_datetime.dt.week\n",
    "    df['month'] = df.pickup_datetime.dt.month\n",
    "    df[\"year\"] = df.pickup_datetime.dt.year\n",
    "    df['day_of_year'] = df.pickup_datetime.dt.dayofyear\n",
    "    df['week_of_year'] = df.pickup_datetime.dt.weekofyear\n",
    "    df[\"weekday\"] = df.pickup_datetime.dt.weekday\n",
    "    df[\"quarter\"] = df.pickup_datetime.dt.quarter\n",
    "    df[\"day_of_month\"] = df.pickup_datetime.dt.day\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = prepare_time_features(train)\n",
    "test = prepare_time_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.shape # add 9 time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate-distance-between-two-latitude-longitude-points-haversine-formula \n",
    "# Returns distance in miles\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = 0.017453292519943295 # Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['distance_miles'] = distance(train.pickup_latitude, train.pickup_longitude, \\\n",
    "                                      train.dropoff_latitude, train.dropoff_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['distance_miles'] = distance(test.pickup_latitude, test.pickup_longitude, \\\n",
    "                                      test.dropoff_latitude, test.dropoff_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[(train['distance_miles']==0)&(train['fare_amount']==0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(train[(train['distance_miles']==0)&(train['fare_amount']==0)].index, axis=0)\n",
    "train.shape # drop 178 obs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating pickup and drop distance from all 3 airports of Air Ports\n",
    "def transform(data):\n",
    "    # Distances to nearby airports, \n",
    "    jfk = (-73.7781, 40.6413)\n",
    "    ewr = (-74.1745, 40.6895)\n",
    "    lgr = (-73.8740, 40.7769)\n",
    "\n",
    "    data['pickup_distance_to_jfk'] = distance(jfk[1], jfk[0],\n",
    "                                         data['pickup_latitude'], data['pickup_longitude'])\n",
    "    data['dropoff_distance_to_jfk'] = distance(jfk[1], jfk[0],\n",
    "                                           data['dropoff_latitude'], data['dropoff_longitude'])\n",
    "    data['pickup_distance_to_ewr'] = distance(ewr[1], ewr[0], \n",
    "                                          data['pickup_latitude'], data['pickup_longitude'])\n",
    "    data['dropoff_distance_to_ewr'] = distance(ewr[1], ewr[0],\n",
    "                                           data['dropoff_latitude'], data['dropoff_longitude'])\n",
    "    data['pickup_distance_to_lgr'] = distance(lgr[1], lgr[0],\n",
    "                                          data['pickup_latitude'], data['pickup_longitude'])\n",
    "    data['dropoff_distance_to_lgr'] = distance(lgr[1], lgr[0],\n",
    "                                           data['dropoff_latitude'], data['dropoff_longitude'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = transform(train)\n",
    "test = transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape # add 7 distance features totally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[train['fare_amount'] < 2.5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the base fare for any taxi in new york is 2.5 dollars, we will drop those cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(train[train['fare_amount'] < 2.5].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('test_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "# lightgbm\n",
    "# xgboost\n",
    "# random forest\n",
    "# catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# two big dataset: sample 5,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 6.84 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train=pd.read_csv('train_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 s, sys: 1.46 s, total: 30.4 s\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train= train.sample(n=20000000)     #å–sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 24)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv('test_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = [f for f in train.columns if f not in ['key','pickup_datetime','fare_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=1001)\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train[feats], train['fare_amount'])):\n",
    "    dtrain = lgb.Dataset(data=train[feats].iloc[train_idx], \n",
    "                         label=train['fare_amount'].iloc[train_idx],\n",
    "                         free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(data=train[feats].iloc[valid_idx],\n",
    "                         label=train['fare_amount'].iloc[valid_idx],\n",
    "                         free_raw_data=False)\n",
    "    params = {'boosting_type': 'gbdt',\n",
    "              'objective': 'regression',\n",
    "              'metric':'rmse',\n",
    "              'learning_rate': 0.03,\n",
    "              'num_leaves': 30, \n",
    "              'max_depth': 7,  \n",
    "              'min_child_samples': 70,  \n",
    "              'max_bin': 300,  \n",
    "              'subsample': 1.0,  \n",
    "              'subsample_freq': 1,  \n",
    "              'colsample_bytree': 0.9,  \n",
    "              'min_split_gain': 0.5,\n",
    "              'min_child_weight': 4,\n",
    "              'reg_lambda':0.1,\n",
    "              'reg_alpha': 0.1,\n",
    "              'nthread': 5,\n",
    "              'verbose': -1,}\n",
    "    \n",
    "    clf = lgb.train(params, \n",
    "                    dtrain, \n",
    "                    valid_sets=[dtrain, dvalid], \n",
    "                    valid_names=['train','valid'],\n",
    "                    num_boost_round=10000,\n",
    "                    early_stopping_rounds=125,\n",
    "                    verbose_eval=500)\n",
    "\n",
    "    oof_preds[valid_idx] = clf.predict(train[feats].iloc[valid_idx])\n",
    "    sub_preds += clf.predict(test[feats]) / folds.n_splits\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('Fold %2d rmse : %.6f' % (n_fold + 1,mean_squared_error(train['fare_amount'].iloc[valid_idx], oof_preds[valid_idx]) ** .5)) \n",
    "    del clf, dtrain, dvalid\n",
    "    gc.collect()\n",
    "\n",
    "print('Full rmse %.6f' % mean_squared_error(train['fare_amount'], oof_preds)**.5)\n",
    "# Write submission file and plot feature importance\n",
    "sub_df = test[['key']].copy()\n",
    "sub_df['fare_amount'] = sub_preds\n",
    "sub_df[['key', 'fare_amount']].to_csv('submission_lgb2_10m.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).index\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = [f for f in train.columns if f not in ['key','pickup_datetime','fare_amount']]\n",
    "X=train[feats]\n",
    "y=train['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   max_bin |   max_depth |   min_child_sample |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample |   subsample_freq | \n",
      "    1 | 00m00s | \u001b[35m  -6.11794\u001b[0m | \u001b[32m            0.9511\u001b[0m | \u001b[32m 485.5402\u001b[0m | \u001b[32m     5.4910\u001b[0m | \u001b[32m           67.3755\u001b[0m | \u001b[32m            5.4100\u001b[0m | \u001b[32m          0.4140\u001b[0m | \u001b[32m     58.3420\u001b[0m | \u001b[32m     0.0814\u001b[0m | \u001b[32m      0.4842\u001b[0m | \u001b[32m     0.8544\u001b[0m | \u001b[32m          6.9699\u001b[0m | \n",
      "    2 | 00m00s |   -6.17495 |             0.6865 |  413.0501 |     -0.0459 |            70.5620 |            14.4540 |           0.4466 |      96.2448 |      0.3746 |       0.8531 |      0.8198 |           7.7255 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   max_bin |   max_depth |   min_child_sample |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample |   subsample_freq | \n",
      "    3 | 00m15s |   -6.25379 |             1.0000 |  180.0000 |      8.0000 |            20.0000 |             3.0000 |           0.1000 |      20.0000 |      0.0000 |       0.0000 |      1.0000 |           1.0000 | \n",
      "    4 | 00m13s |   -6.24336 |             1.0000 |  500.0000 |      8.0000 |           100.0000 |            20.0000 |           0.8000 |      20.0000 |      0.0000 |       1.0000 |      1.0000 |           1.0000 | \n",
      "    5 | 00m21s |   -6.41985 |             0.6000 |  500.0000 |     -1.0000 |            20.0000 |            20.0000 |           0.8000 |     100.0000 |      1.0000 |       1.0000 |      0.6000 |           1.0000 | \n",
      "    6 | 00m22s | \u001b[35m  -6.09754\u001b[0m | \u001b[32m            1.0000\u001b[0m | \u001b[32m 180.0000\u001b[0m | \u001b[32m     8.0000\u001b[0m | \u001b[32m          100.0000\u001b[0m | \u001b[32m            3.0000\u001b[0m | \u001b[32m          0.1000\u001b[0m | \u001b[32m    100.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \u001b[32m      0.0000\u001b[0m | \u001b[32m     0.6000\u001b[0m | \u001b[32m         10.0000\u001b[0m | \n",
      "    7 | 00m24s |   -6.25379 |             1.0000 |  500.0000 |      8.0000 |           100.0000 |             3.0000 |           0.1000 |     100.0000 |      0.0000 |       0.0000 |      1.0000 |          10.0000 | \n",
      "    8 | 00m26s |   -6.09754 |             1.0000 |  387.8801 |      8.0000 |            20.0000 |             3.0000 |           0.1000 |      20.0000 |      0.0000 |       0.0000 |      0.6000 |          10.0000 | \n",
      "    9 | 00m28s |   -6.33697 |             0.6000 |  180.0000 |     -1.0000 |           100.0000 |             3.0000 |           0.8000 |      20.0000 |      1.0000 |       1.0000 |      1.0000 |          10.0000 | \n",
      "   10 | 00m30s |   -6.25379 |             1.0000 |  500.0000 |     -1.0000 |            20.0000 |             3.0000 |           0.1000 |      20.0000 |      0.0000 |       0.0000 |      1.0000 |          10.0000 | \n"
     ]
    }
   ],
   "source": [
    "def bayes_parameter_opt_lgb(X, y, init_round=2, opt_round=8, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.03, output_process=True):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y,free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(num_leaves, colsample_bytree, subsample, max_depth, reg_lambda, reg_alpha, min_split_gain, min_child_weight, \n",
    "                min_child_sample, max_bin, subsample_freq):\n",
    "        params = {'objective':'regression','boosting_type': 'gbdt','nthread': 4, 'verbose': -1,\\\n",
    "                  'num_boost_round': n_estimators, 'learning_rate':learning_rate, \\\n",
    "                  'early_stopping_round':125}\n",
    "        params['subsample_freq']=int(round(subsample_freq))\n",
    "        params['min_child_sample']=int(round(min_child_sample))\n",
    "        params['max_bin']=int(round(max_bin))\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['reg_lambda'] = max(reg_lambda, 0)\n",
    "        params['reg_alpha'] = max(reg_alpha, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=False, verbose_eval=500, metrics=['rmse'])\n",
    "        return -1.0 * np.mean(cv_result['rmse-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (20, 100),\n",
    "                                            'colsample_bytree': (0.6, 1.0),\n",
    "                                            'subsample': (0.6, 1.0),\n",
    "                                            'max_depth': (-1, 8),\n",
    "                                            'reg_lambda': (0, 1),\n",
    "                                            'reg_alpha': (0, 1),\n",
    "                                            'min_child_sample':(20,100),\n",
    "                                            'max_bin':(180,500),\n",
    "                                            'subsample_freq':(1,10),\n",
    "                                            'min_split_gain': (0.1, 0.8),\n",
    "                                            'min_child_weight': (3, 20)})\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    #return lgbBO.res['max']['max_params']\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(X, y, init_round=2, opt_round=8, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.03,output_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=3560)\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train[feats], train['fare_amount'])):\n",
    "    dtrain = lgb.Dataset(data=train[feats].iloc[train_idx], \n",
    "                         label=train['fare_amount'].iloc[train_idx],\n",
    "                         free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(data=train[feats].iloc[valid_idx],\n",
    "                         label=train['fare_amount'].iloc[valid_idx],\n",
    "                         free_raw_data=False)\n",
    "    params = {'boosting_type': 'gbdt',\n",
    "              'objective': 'regression',\n",
    "              'metric':'rmse',\n",
    "              'learning_rate': 0.03,\n",
    "              'num_leaves': 25, \n",
    "              'max_depth': 7,  \n",
    "              'min_child_samples': 36,  \n",
    "              'max_bin': 333,  \n",
    "              'subsample': 0.9525,  \n",
    "              'subsample_freq': 3,  \n",
    "              'colsample_bytree': 0.6416,  \n",
    "              'min_split_gain': 0.1421,\n",
    "              'min_child_weight': 4.0567,\n",
    "              'reg_lambda':0.3254,\n",
    "              'reg_alpha': 0.6913,\n",
    "              'nthread': 8,\n",
    "              'verbose': -1,}\n",
    "    \n",
    "    clf = lgb.train(params, \n",
    "                    dtrain, \n",
    "                    valid_sets=[dtrain, dvalid], \n",
    "                    valid_names=['train','valid'],\n",
    "                    num_boost_round=20000,\n",
    "                    early_stopping_rounds=125,\n",
    "                    verbose_eval=500)\n",
    "\n",
    "    oof_preds[valid_idx] = clf.predict(train[feats].iloc[valid_idx])\n",
    "    sub_preds += clf.predict(test[feats]) / folds.n_splits\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('Fold %2d rmse : %.6f' % (n_fold + 1,mean_squared_error(train['fare_amount'].iloc[valid_idx], oof_preds[valid_idx]) ** .5)) \n",
    "    del clf, dtrain, dvalid\n",
    "    gc.collect()\n",
    "\n",
    "print('Full rmse %.6f' % mean_squared_error(train['fare_amount'], oof_preds)**.5)\n",
    "# Write submission file and plot feature importance\n",
    "sub_df = test[['key']].copy()\n",
    "sub_df['fare_amount'] = sub_preds\n",
    "sub_df[['key', 'fare_amount']].to_csv('submission_lgb_bayesian.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since distance is very important, we need to add more distance realted features\n",
    "# also some time feature contribute too little, we will consider to remove them from features\n",
    "# anything else: some numeric feature to categorical feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
